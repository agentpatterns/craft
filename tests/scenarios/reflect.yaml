skill: reflect
version: "2026-02-24"
category: behavioral

# Triggering tests for the reflect skill.
#
# The testable unit is the description field:
#   "Post-session reflection skill. Reads recent git history, artifacts, and
#    context files to extract learnings and produce improvement proposals for
#    skills, CLAUDE.md, hooks, and plan templates. Use after any substantive
#    session."
#
# Probe with: "When would you use the reflect skill?"

triggering:
  positive:
    # Direct trigger words from the triggers array
    - "reflect on what we built today"
    - "retrospective on this implementation session"
    - "session reflection — the OAuth feature is done"
    - "what did we learn from this craft session?"

    # Phrases aligned with "extract learnings" and "improvement proposals" intent
    - "let's capture what caused friction during implementation so we can improve the skill"
    - "the session is over — what should we update in CLAUDE.md?"
    - "we just finished the payment refactor — any process improvements to capture?"

    # Phrases emphasizing skill/hook/template improvement
    - "the draft skill felt incomplete — let's propose improvements"
    - "I want to update the skills based on what we learned"
    - "run a reflection to see if any hooks should be added based on today's session"

    # Post-implementation, mine git history intent
    - "look at recent commits and tell me what patterns emerged"

    # Edge case: user finishes a session and wants to close the loop
    - "we're done with the feature — let's do a post-mortem before the next one"

  negative:
    # Should trigger research instead (exploring before building, not after)
    - "research how the existing codebase handles authentication"
    - "explore what patterns we should follow for the new feature"
    - "investigate the plugin architecture before we start"

    # Should trigger draft instead (planning, not reflecting)
    - "plan the next phase of the OAuth feature"
    - "create an implementation roadmap based on the research"
    - "draft the beads task graph for the payment refactor"

    # Should trigger craft instead (building, not reflecting)
    - "implement the feature from the beads task graph"
    - "execute the plan and dispatch the agents"
    - "build from the plan we drafted"

    # Should trigger tdd instead (test-writing, not post-session analysis)
    - "write tests first for the new discount validator"
    - "do TDD on the refund flow"
    - "red green refactor the cart total calculation"

    # Mid-session course corrections — description explicitly excludes this
    - "the skill produced unexpected output — fix it right now"
    - "something went wrong during implementation — let's correct it"

    # Completely unrelated
    - "what does git rebase do?"
    - "explain hexagonal architecture"
    - "fix the broken import in OrderService.ts"

    # Ambiguous edge case: "review" could mean code review, not session reflection
    - "review what we changed in this session"

    # Ambiguous edge case: "lessons learned" from a doc, not a session
    - "what are the general lessons learned about TDD?"

functional:
  - id: reflect-01
    description: Reflection artifact contains Agent Dispatch Manifest table with all 4 required agent rows
    grading: code
    given: |
      A git repository with at least 5 commits. The docs/plans/ directory exists with at least
      one recent plan file. CLAUDE.md is present at the project root. The reflect skill is loaded.
    when: "reflect on the session — we just finished implementing the OAuth feature"
    then:
      - "Reflection artifact exists at docs/plans/YYYY-MM-DD-*-reflection.md"
      - "Artifact contains section: ## Agent Dispatch Manifest"
      - "Agent Dispatch Manifest table has exactly 4 rows (Git Historian, Artifact Scout, Context Reader, Skill Inspector)"
      - "Each table row has a non-empty Key Finding column (not a placeholder like '{1-line summary}')"
      - "All 4 agents show Status: completed"
    notes: "Haiku may dispatch agents sequentially rather than in a single message — watch for 4 separate Task calls instead of one batched call"

  - id: reflect-02
    description: Improvement proposals are capped at 5, priority-ordered, and each contains all required fields
    grading: code
    given: |
      A project with git history showing a recent craft session. The session involved multiple
      tool calls, at least one re-try or correction, and a final passing test suite. The reflect
      skill is loaded.
    when: "retrospective — we just finished the payment refactor session and there was noticeable friction around how the craft skill handles remediation"
    then:
      - "Reflection artifact contains a proposals section (## Improvement Proposals or ## Proposals)"
      - "Number of proposals is between 1 and 5 (inclusive)"
      - "Proposals are ordered by priority (P1 before P2 before P3)"
      - "Each proposal includes a Type field (one of: skill-update, claude-md, hook, plan-template, new-skill)"
      - "Each proposal includes a Priority field (P1, P2, or P3)"
      - "Each proposal includes a Target file field with a file path"
      - "Each proposal includes a Current state section with quoted text"
      - "Each proposal includes a Proposed change section"
      - "Each proposal includes a Rationale section"
    notes: "Sonnet may generate 6+ proposals if the session context is rich — enforce the 5-proposal cap explicitly"

  - id: reflect-03
    description: Improvement proposals are actionable, relevant to the session, and avoid invented problems
    grading: llm-judge
    given: |
      A project with the following simulated session context:
      - git log shows 3 commits in the last hour: "craft: implement discount validator", "craft: fix failing test for edge case", "craft: add integration test"
      - craft-execution-log.md shows one remediation cycle (a lint error that was auto-fixed)
      - No existing AskUserQuestion hook in .claude/settings.json
      - The craft skill's SKILL.md does not mention the lint fast-path behavior in its anti-patterns section
    when: "reflect on the craft session — the lint auto-fix step caused confusion because it wasn't documented anywhere"
    then:
      - "Proposals reference artifacts actually present in the session context (craft-execution-log.md, git commits, SKILL.md)"
      - "At least one proposal targets the craft skill or CLAUDE.md, not an unrelated file"
      - "No proposal describes a problem that cannot be inferred from the provided session context"
      - "Proposals are specific and actionable (not generic advice like 'improve documentation')"
    judge_prompt: |
      You are evaluating a reflect skill output. The session context included:
      - 3 craft commits (implement, fix failing test, add integration test)
      - One lint auto-fix remediation cycle documented in craft-execution-log.md
      - The lint fast-path behavior is absent from the craft SKILL.md anti-patterns section
      - No AskUserQuestion hook exists in .claude/settings.json

      Score each dimension from 1-3, then compute the total. Pass threshold: 9 out of 12.

      Dimension 1 — Relevance (1-3): Are proposals grounded in the actual session context?
        1 = Proposals are generic or reference files not in the session context
        2 = Most proposals reference session artifacts but one or more feel invented
        3 = All proposals are directly traceable to session evidence

      Dimension 2 — Actionability (1-3): Are proposals specific enough to implement without further clarification?
        1 = Proposals are vague ("improve the skill", "add more context")
        2 = Most proposals are actionable but one is underspecified
        3 = Every proposal has a clear Target file and a Proposed change that could be implemented immediately

      Dimension 3 — Anti-pattern adherence (1-3): Does the output avoid the anti-patterns listed in the skill?
        1 = Multiple anti-patterns violated (invented problems, bundled proposals, sweeping rewrites)
        2 = One anti-pattern violation (e.g., slightly over-broad proposal)
        3 = No anti-pattern violations; proposals are small and targeted

      Dimension 4 — Proposal count (1-3): Is the number of proposals appropriate?
        1 = More than 5 proposals (violates the cap constraint)
        2 = Exactly 5 proposals (at the cap, acceptable)
        3 = 3-4 proposals (focused; signal over noise)

      Total score (sum of all dimensions): pass if >= 9 out of 12.
      Return: PASS or FAIL, the total score, and one sentence per dimension explaining the score.
